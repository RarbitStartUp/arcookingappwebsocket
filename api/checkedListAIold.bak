// checkedListAI.js (server-side logic for processing checked lists)

import { VertexAI } from "@google-cloud/vertexai";
import sharp from 'sharp';
import fs from 'fs';

const project = "arcookingapp";
const location = "us-central1";
const vertex_ai = new VertexAI({ project, location });

const generativeVisionModel = vertex_ai.preview.getGenerativeModel({
  model: "gemini-pro-vision",
});

// Function to process checked list and return the result
export async function checkedListAI(jsonData, frames) {
  return new Promise(async (resolve, reject) => {
  console.log("Entering checkedListAI function");
  console.log("frames :", frames );
  const framesData = frames[0].data
  console.log( "framesData :",  framesData )

     // Convert frames raw pixels data to base64
     const binaryData = new Uint8Array(framesData);
  console.log( "binaryData :",  binaryData )


     // Convert raw pixels to a JPEG buffer using sharp
//  const jpegBuffer = await sharp(Buffer.from(binaryData), {
//    raw: {
//      width: 640, // Adjust width based on your actual frame width
//      height: 480, // Adjust height based on your actual frame height
//      channels: 3, // Assuming 3 channels (RGB), adjust if needed
//    },
//  }).jpeg({ quality: 80 }).toBuffer();
 const jpegBuffer = await sharp(Buffer.from(binaryData), {
   raw: {
     width: 640, // Adjust width based on your actual frame width
     height: 480, // Adjust height based on your actual frame height
     channels: 3, // Assuming 3 channels (RGB), adjust if needed
   },
 }).jpeg({ quality: 80 }).toBuffer({ resolveWithObject: true, limitInputPixels: false });

 console.log( "jpegBuffer :",  jpegBuffer )

// Save the JPEG buffer to a file
fs.writeFileSync('output.jpg', jpegBuffer.data);

console.log('JPEG image saved to output.jpg');

 
 // Convert the JPEG buffer to base64
 const base64Data = jpegBuffer.toString('base64');
//  console.log( "base64Data :",  base64Data )
 console.log("Base64 Content Length:", base64Data.length);
  
  try {
    const prompt = `
      You are an action detection AI, 
      check if the objects and actions in the user's JSON checklist are detected in the live stream from the camera,
      User's JSON CheckList :
      ( only check the objects and actions marked "true", you don't need to check the objects and actions marked false )
      ${JSON.stringify(jsonData)}

      and return the new checklist below JSON format :
      {
        checklist: {
          objects:{
            "objects": true | false
          },
          actions:{
            "actions": true | false
          }
        },
      }
    `;

    // Convert frames to base64
    // const base64Frames = frames.map((frame) =>
    //   arrayBufferToBase64(frame.data.buffer)
    // );

    // Convert frames raw pixels data to base64
    // Check if framesData is an array before attempting to map
// if (Array.isArray(framesData)) {
//   const base64Frames = framesData.map(frame => {
//     const binaryData = new Uint8Array(frame.data);
//     const base64Data = btoa(String.fromCharCode(...binaryData));
//     return base64Data;
//   });
//   // Continue with the rest of your code...
// } else {
//   console.error("framesData is not an array");
// }

 

    // const base64Data = btoa(String.fromCharCode.apply(null, binaryData));
    // const base64Data = Buffer.from(binaryData).toString("base64");
    // const base64Data = btoa(String.fromCharCode(...binaryData));

    // Send base64-encoded frames to Vertex AI for detection
    const filePart = {
      file_data: {
        file_content: base64Data, // Send frames as base64-encoded content
        mime_type: "image/jpeg", // Adjust the mime type as needed
        // mime_type: "application/octet-stream", // Adjust the mime type as needed
        // file_content: framesData, // Send frames as base64-encoded content
      },
    };

    const textPart = { text: prompt };
    const request = {
      contents: [{ role: "user", parts: [textPart, filePart] }],
    };

    console.log("Before generativeVisionModel.generateContentStream");
    const streamingResp = await generativeVisionModel.generateContentStream(
      request
    );
    console.log("streamingResp:", streamingResp);
    console.log("After generativeVisionModel.generateContentStream");

    const aggregatedResponse = await streamingResp.response;

    console.log("After awaiting response from AI API");

    // Return the generated content
    // return aggregatedResponse.candidates[0].content;
    resolve(aggregatedResponse.candidates[0].content);
  } catch (error) {
    console.error("Error in generativeVisionModel.generateContentStream:", error);
    console.error("Error details:", error.response?.data || error.message);
    reject(error);
    // console.error("Error in checkedListAI function:", error);
    // reject("Error generating content");

    if (error.response && error.response.data) {
      console.log("Error response data:", error.response.data);
    } else {
      console.log("Error details:", error.message);
    }
    return "Error generating content";
  
  }
})
}

// Function to convert array buffer to base64
// function arrayBufferToBase64(buffer) {
//   let binary = "";
//   const bytes = new Uint8Array(buffer);
//   for (let i = 0; i < bytes.byteLength; i++) {
//     binary += String.fromCharCode(bytes[i]);
//   }
//   return btoa(binary);
// }


